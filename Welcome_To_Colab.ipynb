{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rizvi999/neural-lab/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ‚ö° LSTM Named Entity Recognition (NER)\n",
        "# with GloVe embeddings + Balanced Dataset\n",
        "# =========================================\n",
        "!pip install gensim --quiet\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import gensim.downloader as api\n",
        "\n",
        "# ------------------------------\n",
        "# Step 1: Prepare Expanded Dataset\n",
        "# ------------------------------\n",
        "\n",
        "sentences = [\n",
        "    [\"john\", \"lives\", \"in\", \"new\", \"york\", \"city\"],\n",
        "    [\"mary\", \"works\", \"at\", \"google\"],\n",
        "    [\"bob\", \"studies\", \"at\", \"stanford\", \"university\"],\n",
        "    [\"alice\", \"is\", \"from\", \"paris\"],\n",
        "    [\"david\", \"works\", \"in\", \"london\"],\n",
        "    [\"emma\", \"is\", \"a\", \"doctor\"],\n",
        "    [\"sam\", \"lives\", \"in\", \"delhi\"],\n",
        "    [\"rizvi\", \"works\", \"at\", \"microsoft\"],\n",
        "    [\"rahul\", \"studies\", \"in\", \"mumbai\", \"university\"],\n",
        "    [\"aarav\", \"is\", \"from\", \"chennai\"],\n",
        "\n",
        "    # Added more diverse examples\n",
        "    [\"john\", \"works\", \"at\", \"microsoft\"],\n",
        "    [\"emma\", \"studies\", \"in\", \"oxford\"],\n",
        "    [\"mary\", \"lives\", \"in\", \"paris\"],\n",
        "    [\"bob\", \"works\", \"at\", \"amazon\"],\n",
        "    [\"alice\", \"studies\", \"in\", \"delhi\"],\n",
        "    [\"sara\", \"works\", \"at\", \"ibm\"],\n",
        "    [\"george\", \"is\", \"from\", \"rome\"],\n",
        "    [\"james\", \"studies\", \"in\", \"tokyo\"],\n",
        "    [\"ravi\", \"works\", \"at\", \"infosys\"],\n",
        "    [\"nina\", \"lives\", \"in\", \"mumbai\"]\n",
        "]\n",
        "\n",
        "tags = [\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-ORG\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"I-ORG\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-ORG\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\", \"I-ORG\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-ORG\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-ORG\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-ORG\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-ORG\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"]\n",
        "]\n",
        "\n",
        "# ------------------------------\n",
        "# Step 2: Create Vocabulary\n",
        "# ------------------------------\n",
        "words = list(set(w for s in sentences for w in s))\n",
        "tags_vocab = list(set(t for ts in tags for t in ts))\n",
        "\n",
        "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
        "word2idx[\"PAD\"] = 0\n",
        "word2idx[\"UNK\"] = 1\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "tag2idx = {t: i + 1 for i, t in enumerate(tags_vocab)}\n",
        "tag2idx[\"PAD\"] = 0\n",
        "idx2tag = {i: t for t, i in tag2idx.items()}\n",
        "\n",
        "max_len = 10\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Prepare Input Data\n",
        "# ------------------------------\n",
        "X = [[word2idx.get(w, 1) for w in s] for s in sentences]\n",
        "X = pad_sequences(X, maxlen=max_len, padding=\"post\", value=0)\n",
        "\n",
        "y = [[tag2idx[t] for t in ts] for ts in tags]\n",
        "y = pad_sequences(y, maxlen=max_len, padding=\"post\", value=0)\n",
        "y = [to_categorical(i, num_classes=len(tag2idx)) for i in y]\n",
        "\n",
        "# ------------------------------\n",
        "# Step 4: Load GloVe Embeddings\n",
        "# ------------------------------\n",
        "print(\"üîπ Loading GloVe embeddings (50D)...\")\n",
        "glove_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2idx), 50))\n",
        "for word, i in word2idx.items():\n",
        "    if word in glove_vectors:\n",
        "        embedding_matrix[i] = glove_vectors[word]\n",
        "print(\"‚úÖ GloVe embeddings loaded successfully!\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step 5: Build BiLSTM Model\n",
        "# ------------------------------\n",
        "input_word = Input(shape=(max_len,))\n",
        "model = Embedding(\n",
        "    input_dim=len(word2idx),\n",
        "    output_dim=50,\n",
        "    weights=[embedding_matrix],\n",
        "    input_length=max_len,\n",
        "    trainable=False\n",
        ")(input_word)\n",
        "model = Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))(model)\n",
        "out = TimeDistributed(Dense(len(tag2idx), activation=\"softmax\"))(model)\n",
        "\n",
        "model = Model(input_word, out)\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# ------------------------------\n",
        "# Step 6: Train Model\n",
        "# ------------------------------\n",
        "print(\"\\nüöÄ Training model...\\n\")\n",
        "model.fit(np.array(X), np.array(y), batch_size=2, epochs=40, verbose=1)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 7: Prediction Function\n",
        "# ------------------------------\n",
        "common_words = {\"in\", \"at\", \"is\", \"a\", \"the\", \"works\", \"studies\", \"from\", \"lives\"}\n",
        "\n",
        "def predict_sentence(sentence):\n",
        "    words_in = sentence.lower().split()\n",
        "    x = pad_sequences([[word2idx.get(w, 1) for w in words_in]], maxlen=max_len, padding=\"post\", value=0)\n",
        "    pred = model.predict(x, verbose=0)[0]\n",
        "    pred_tags = [idx2tag[np.argmax(p)] for p in pred][:len(words_in)]\n",
        "\n",
        "    # Fix common function words to \"O\"\n",
        "    for i, w in enumerate(words_in):\n",
        "        if w in common_words:\n",
        "            pred_tags[i] = \"O\"\n",
        "\n",
        "    print(f\"\\nüîÆ Prediction for: {sentence}\")\n",
        "    print(\"---------------------------------------------\")\n",
        "    for w, t in zip(words_in, pred_tags):\n",
        "        print(f\"{w:<12} ---> {t}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step 8: Test the Model\n",
        "# ------------------------------\n",
        "predict_sentence(\"sam lives in delhi\")\n",
        "predict_sentence(\"rizvi works at google\")\n",
        "predict_sentence(\"mary is from paris\")\n",
        "predict_sentence(\"john studies in london\")\n",
        "predict_sentence(\"george works at amazon\")\n",
        "predict_sentence(\"emma lives in rome\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugqS9_0LicHX",
        "outputId": "b6ceab2a-8c7c-44e6-c026-edac06ccf600"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hüîπ Loading GloVe embeddings (50D)...\n",
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "‚úÖ GloVe embeddings loaded successfully!\n",
            "\n",
            "üöÄ Training model...\n",
            "\n",
            "Epoch 1/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.3137 - loss: 1.8635\n",
            "Epoch 2/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7755 - loss: 1.5097\n",
            "Epoch 3/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7959 - loss: 1.0378\n",
            "Epoch 4/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8330 - loss: 0.6275\n",
            "Epoch 5/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8839 - loss: 0.4921\n",
            "Epoch 6/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8987 - loss: 0.3289\n",
            "Epoch 7/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8901 - loss: 0.3217\n",
            "Epoch 8/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9560 - loss: 0.2444\n",
            "Epoch 9/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9512 - loss: 0.3249\n",
            "Epoch 10/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9753 - loss: 0.1755\n",
            "Epoch 11/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9492 - loss: 0.2078\n",
            "Epoch 12/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9731 - loss: 0.1453\n",
            "Epoch 13/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9770 - loss: 0.1156\n",
            "Epoch 14/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9823 - loss: 0.1073\n",
            "Epoch 15/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9726 - loss: 0.1077\n",
            "Epoch 16/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9847 - loss: 0.0861\n",
            "Epoch 17/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9745 - loss: 0.0892\n",
            "Epoch 18/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9871 - loss: 0.0683\n",
            "Epoch 19/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9725 - loss: 0.0934\n",
            "Epoch 20/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9879 - loss: 0.0627\n",
            "Epoch 21/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9966 - loss: 0.0565\n",
            "Epoch 22/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0363\n",
            "Epoch 23/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0405\n",
            "Epoch 24/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9908 - loss: 0.0628\n",
            "Epoch 25/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0277\n",
            "Epoch 26/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0245\n",
            "Epoch 27/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0274\n",
            "Epoch 28/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0241\n",
            "Epoch 29/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9946 - loss: 0.0282\n",
            "Epoch 30/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0228\n",
            "Epoch 31/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0189\n",
            "Epoch 32/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0177\n",
            "Epoch 33/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0181\n",
            "Epoch 34/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0167\n",
            "Epoch 35/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0121\n",
            "Epoch 36/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0130\n",
            "Epoch 37/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0148\n",
            "Epoch 38/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0180\n",
            "Epoch 39/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0149\n",
            "Epoch 40/40\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b36c69d7560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÆ Prediction for: sam lives in delhi\n",
            "---------------------------------------------\n",
            "sam          ---> B-PER\n",
            "lives        ---> O\n",
            "in           ---> O\n",
            "delhi        ---> B-LOC\n",
            "\n",
            "üîÆ Prediction for: rizvi works at google\n",
            "---------------------------------------------\n",
            "rizvi        ---> B-PER\n",
            "works        ---> O\n",
            "at           ---> O\n",
            "google       ---> B-ORG\n",
            "\n",
            "üîÆ Prediction for: mary is from paris\n",
            "---------------------------------------------\n",
            "mary         ---> B-PER\n",
            "is           ---> O\n",
            "from         ---> O\n",
            "paris        ---> B-LOC\n",
            "\n",
            "üîÆ Prediction for: john studies in london\n",
            "---------------------------------------------\n",
            "john         ---> B-PER\n",
            "studies      ---> O\n",
            "in           ---> O\n",
            "london       ---> B-LOC\n",
            "\n",
            "üîÆ Prediction for: george works at amazon\n",
            "---------------------------------------------\n",
            "george       ---> B-PER\n",
            "works        ---> O\n",
            "at           ---> O\n",
            "amazon       ---> B-ORG\n",
            "\n",
            "üîÆ Prediction for: emma lives in rome\n",
            "---------------------------------------------\n",
            "emma         ---> B-PER\n",
            "lives        ---> O\n",
            "in           ---> O\n",
            "rome         ---> B-LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Neural Machine Translation with Attention (Fast & Runnable)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "!pip install datasets tensorflow --quiet\n",
        "\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Load small English‚ÄìFrench dataset\n",
        "# ------------------------------------------------------------\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
        "\n",
        "pairs = [(ex[\"translation\"][\"en\"], ex[\"translation\"][\"fr\"]) for ex in dataset[\"train\"][:5000]]\n",
        "eng_texts = [p[0] for p in pairs]\n",
        "fra_texts = [f\"<sos> {p[1]} <eos>\" for p in pairs]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ Tokenize + Pad\n",
        "# ------------------------------------------------------------\n",
        "max_eng_len = 20\n",
        "max_fra_len = 20\n",
        "\n",
        "eng_tokenizer = Tokenizer(filters='', lower=True)\n",
        "fra_tokenizer = Tokenizer(filters='', lower=True)\n",
        "eng_tokenizer.fit_on_texts(eng_texts)\n",
        "fra_tokenizer.fit_on_texts(fra_texts)\n",
        "\n",
        "eng_vocab = len(eng_tokenizer.word_index) + 1\n",
        "fra_vocab = len(fra_tokenizer.word_index) + 1\n",
        "\n",
        "encoder_input = pad_sequences(eng_tokenizer.texts_to_sequences(eng_texts), maxlen=max_eng_len, padding='post')\n",
        "decoder_input = pad_sequences(fra_tokenizer.texts_to_sequences(fra_texts), maxlen=max_fra_len, padding='post')\n",
        "\n",
        "decoder_target = np.zeros_like(decoder_input)\n",
        "decoder_target[:, :-1] = decoder_input[:, 1:]\n",
        "\n",
        "X_train_enc, X_val_enc, X_train_dec, X_val_dec, y_train, y_val = train_test_split(\n",
        "    encoder_input, decoder_input, decoder_target, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ Build Encoder‚ÄìDecoder with Attention\n",
        "# ------------------------------------------------------------\n",
        "latent_dim = 256\n",
        "\n",
        "# Encoder\n",
        "enc_inputs = Input(shape=(max_eng_len,))\n",
        "enc_emb = Embedding(eng_vocab, latent_dim, mask_zero=True)(enc_inputs)\n",
        "enc_out, enc_h, enc_c = LSTM(latent_dim, return_sequences=True, return_state=True)(enc_emb)\n",
        "\n",
        "# Decoder\n",
        "dec_inputs = Input(shape=(max_fra_len,))\n",
        "dec_emb = Embedding(fra_vocab, latent_dim, mask_zero=True)(dec_inputs)\n",
        "dec_lstm_out, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True)(dec_emb, initial_state=[enc_h, enc_c])\n",
        "\n",
        "# Attention\n",
        "attn = Attention()([dec_lstm_out, enc_out])\n",
        "concat = tf.keras.layers.Concatenate()([dec_lstm_out, attn])\n",
        "dec_dense = Dense(fra_vocab, activation='softmax')(concat)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], dec_dense)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ Train quickly (only 3 epochs)\n",
        "# ------------------------------------------------------------\n",
        "model.fit(\n",
        "    [X_train_enc, X_train_dec],\n",
        "    y_train.reshape(y_train.shape[0], y_train.shape[1], 1),\n",
        "    validation_data=([X_val_enc, X_val_dec], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)),\n",
        "    batch_size=64,\n",
        "    epochs=3\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training complete!\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5Ô∏è‚É£ Define inference model (simplified greedy decoding)\n",
        "# ------------------------------------------------------------\n",
        "encoder_model = Model(enc_inputs, [enc_out, enc_h, enc_c])\n",
        "\n",
        "dec_state_input_h = Input(shape=(latent_dim,))\n",
        "dec_state_input_c = Input(shape=(latent_dim,))\n",
        "enc_out_input = Input(shape=(max_eng_len, latent_dim))\n",
        "\n",
        "dec_emb2 = dec_emb(dec_inputs)\n",
        "dec_out2, dec_h2, dec_c2 = LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
        "    dec_emb2, initial_state=[dec_state_input_h, dec_state_input_c]\n",
        ")\n",
        "attn2 = Attention()([dec_out2, enc_out_input])\n",
        "concat2 = tf.keras.layers.Concatenate()([dec_out2, attn2])\n",
        "dec_pred2 = dec_dense(concat2)\n",
        "decoder_model = Model(\n",
        "    [dec_inputs, enc_out_input, dec_state_input_h, dec_state_input_c],\n",
        "    [dec_pred2, dec_h2, dec_c2]\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6Ô∏è‚É£ Translation function\n",
        "# ------------------------------------------------------------\n",
        "reverse_fra_index = {i: w for w, i in fra_tokenizer.word_index.items()}\n",
        "\n",
        "def translate(sentence):\n",
        "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_eng_len, padding='post')\n",
        "    enc_outs, enc_h, enc_c = encoder_model.predict(seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = fra_tokenizer.word_index['<sos>']\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    for _ in range(max_fra_len):\n",
        "        preds, h, c = decoder_model.predict([target_seq, enc_outs, enc_h, enc_c])\n",
        "        idx = np.argmax(preds[0, -1, :])\n",
        "        word = reverse_fra_index.get(idx, '')\n",
        "        if word == '<eos>' or word == '':\n",
        "            break\n",
        "        decoded_sentence += ' ' + word\n",
        "        target_seq[0, 0] = idx\n",
        "        enc_h, enc_c = h, c\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7Ô∏è‚É£ Try sample translations\n",
        "# ------------------------------------------------------------\n",
        "for sent in [\"good night\", \"how are you\", \"i love books\", \"she is a teacher\"]:\n",
        "    print(f\"\\nüó£Ô∏è English: {sent}\")\n",
        "    print(f\"üá´üá∑ French (predicted): {translate(sent)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cKzQYTm7pEt6",
        "outputId": "151af296-e57e-4be1-c62e-693b735a6a95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers, not 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-850131478.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"opus_books\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en-fr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"translation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"translation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0meng_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mfra_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"<sos> {p[1]} <eos>\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}