{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rizvi999/neural-lab/blob/main/11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Device setup\n",
        "# -------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Download and preprocess text\n",
        "# -------------------------------\n",
        "url = 'https://www.gutenberg.org/files/1342/1342-0.txt'  # Pride and Prejudice\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Preprocess text: lowercase, only letters and space\n",
        "text = text.lower()\n",
        "text = ''.join(c for c in text if c.isalpha() or c.isspace())\n",
        "\n",
        "# Reduce corpus size for faster training\n",
        "max_chars = 100000\n",
        "text = text[:max_chars]\n",
        "\n",
        "# Character mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char2idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "idx2char = {idx: ch for idx, ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Convert text to integers\n",
        "text_as_int = np.array([char2idx[c] for c in text], dtype=np.int64)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define LSTM model\n",
        "# -------------------------------\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embed(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, hidden\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Batch generator\n",
        "# -------------------------------\n",
        "def batch_generator(text_as_int, seq_length, batch_size):\n",
        "    text_len = len(text_as_int)\n",
        "    while True:\n",
        "        X_batch = []\n",
        "        y_batch = []\n",
        "        for _ in range(batch_size):\n",
        "            start_idx = np.random.randint(0, text_len - seq_length - 1)\n",
        "            X_batch.append(text_as_int[start_idx:start_idx + seq_length])\n",
        "            y_batch.append(text_as_int[start_idx + seq_length])\n",
        "        X_batch = torch.tensor(np.array(X_batch, dtype=np.int64), dtype=torch.long).to(device)\n",
        "        y_batch = torch.tensor(np.array(y_batch, dtype=np.int64), dtype=torch.long).to(device)\n",
        "        yield X_batch, y_batch\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Training setup\n",
        "# -------------------------------\n",
        "embed_size = 64\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "seq_length = 50\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "steps_per_epoch = 100\n",
        "learning_rate = 0.005\n",
        "\n",
        "model = CharLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "generator = batch_generator(text_as_int, seq_length, batch_size)\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Train the model\n",
        "# -------------------------------\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for step in range(steps_per_epoch):\n",
        "        X_batch, y_batch = next(generator)\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(X_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/steps_per_epoch:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Optimized autocomplete function\n",
        "# -------------------------------\n",
        "def autocomplete(model, start_text, char2idx, idx2char, predict_len=100, temperature=0.5):\n",
        "    model.eval()\n",
        "    chars = [ch for ch in start_text]\n",
        "    input_seq = torch.tensor([[char2idx[ch] for ch in chars]], dtype=torch.long).to(device)\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            probs = F.softmax(output / temperature, dim=-1).squeeze()\n",
        "            char_idx = torch.multinomial(probs, 1).item()\n",
        "            chars.append(idx2char[char_idx])\n",
        "            input_seq = torch.tensor([[char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Interactive autocomplete loop\n",
        "# -------------------------------\n",
        "print(\"\\nOptimized character-level LSTM autocomplete is ready! Type 'quit' to exit.\\n\")\n",
        "while True:\n",
        "    user_input = input(\"Type some text: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "    if len(user_input) < 1:\n",
        "        print(\"Type at least one character!\")\n",
        "        continue\n",
        "\n",
        "    prediction = autocomplete(model, user_input[-seq_length:], char2idx, idx2char, predict_len=100, temperature=0.5)\n",
        "    print(\"Autocomplete suggestion:\\n\", prediction)\n",
        "    print()"
      ],
      "metadata": {
        "id": "ml_jVyUqi7xs",
        "outputId": "d7c43c75-3b8d-47a1-e99c-ca6c90c5f5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocabulary size: 33\n",
            "Epoch [1/30], Loss: 2.3705\n",
            "Epoch [2/30], Loss: 1.8627\n",
            "Epoch [3/30], Loss: 1.7076\n",
            "Epoch [4/30], Loss: 1.6197\n",
            "Epoch [5/30], Loss: 1.5563\n",
            "Epoch [6/30], Loss: 1.5053\n",
            "Epoch [7/30], Loss: 1.4315\n",
            "Epoch [8/30], Loss: 1.4147\n",
            "Epoch [9/30], Loss: 1.3778\n",
            "Epoch [10/30], Loss: 1.3771\n",
            "Epoch [11/30], Loss: 1.3584\n",
            "Epoch [12/30], Loss: 1.3355\n",
            "Epoch [13/30], Loss: 1.3169\n",
            "Epoch [14/30], Loss: 1.3238\n",
            "Epoch [15/30], Loss: 1.2814\n",
            "Epoch [16/30], Loss: 1.2813\n",
            "Epoch [17/30], Loss: 1.2672\n",
            "Epoch [18/30], Loss: 1.2634\n",
            "Epoch [19/30], Loss: 1.2324\n",
            "Epoch [20/30], Loss: 1.2535\n",
            "Epoch [21/30], Loss: 1.2521\n",
            "Epoch [22/30], Loss: 1.2397\n",
            "Epoch [23/30], Loss: 1.2391\n",
            "Epoch [24/30], Loss: 1.2189\n",
            "Epoch [25/30], Loss: 1.2006\n",
            "Epoch [26/30], Loss: 1.2074\n",
            "Epoch [27/30], Loss: 1.2013\n",
            "Epoch [28/30], Loss: 1.2150\n",
            "Epoch [29/30], Loss: 1.2090\n",
            "Epoch [30/30], Loss: 1.1910\n",
            "\n",
            "Optimized character-level LSTM autocomplete is ready! Type 'quit' to exit.\n",
            "\n",
            "Type some text: hello world \n",
            "Autocomplete suggestion:\n",
            " hello world her admiration of the gentleman and a bennets one of the nothing a man and the that a great deal of \n",
            "\n",
            "Type some text: welcome home\n",
            "Autocomplete suggestion:\n",
            " welcome homes a very before than it will to me disliked his own she had not an exquisions that she was likely an\n",
            "\n",
            "Type some text: government\n",
            "Autocomplete suggestion:\n",
            " government the formort than the next darcy on the world\n",
            "distinguls better the real to interted the rest to me\n",
            "\n",
            "Type some text: hi\n",
            "Autocomplete suggestion:\n",
            " his office the rest and mr darcy of\n",
            "with objections the worlds which i am graad the disposed to disco\n",
            "\n",
            "Type some text: super\n",
            "Autocomplete suggestion:\n",
            " supers and her liked an easing the shore of sure you have a preaculy\n",
            "righte is very bennets one and a gr\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}